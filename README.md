# COVID-19 

## 개발 목적
코로나 블루 대처를 위한 스마트 로봇 및 어플 개발
최근에 전국 269만 초중고학생들이 학교에서 수업을 받고 있지만 10명 중 1명은 불안해서 또는 의심 증상이 있다는 이유로 학교에 가지 않고 있으며, 학교 전체가 등교를 미룬 곳도 800곳을 넘어서고 있다.

이에 학부모의 불안이 고조되고 있는 현 상황에서 학부모들이 가정 및 직장에서 아이의 건강상태 및 심리상태를 비대면으로 모니터링을 하고, 아이들에게 친숙한 펭수로봇을 통해 질병관리본부에서 제작한 감염병예방수칙을 홍보하면서 아이들의 표정변화 등을 빅데이터 및 AI 기술을 적용한 시제품을 개발하기로 하였다.

##  개발 중점 
● H/W시스템 : 엣지컴퓨팅으로 AI를 설계 및 구현
­ AI기반의 안면인식 시스템 설계 및 구현
    - 표정변화를 인식하기 위한 트래킹기술 구현
    - 로봇의 기능제어를 위한 제어 설계 및 구현
    - 라즈베리파이 4B 기반의 Ubuntu OS Build
    - 자율주행이 가능한 초음파센서를 이용한 제어 설계 및 구현
    - 영상을 분석하기 위한 영상획득 및 정제
    - 열감지(적외선) 센서를 이용한 열감지 측정 설계 및 구현
    - Wifi를 이용한 영상스트리밍 전송 시스템 설계 및 구현
    - 디스플레이를 활용한 저장정보 재생 시스템 설계 및 구현
    - 음성인식데이터를 Text로 전환하는 시스템 설계 및 구현

● S/W시스템 : 비대면 상담 및 치료 스마트폰 및 디스플레이용 앱개발
    - Cross-Platform인 Kivy를 활용한 UI/UX 설계 및 구현
    - 로봇에서 전송되는 정보를 활용한 빅데이터 시각화 설계 및 구현
    - 얼굴표정인식 정보를 활용한 표정분석 패턴분석 설계 및 구현
    - 아이들의 그림정보를 획득하여 AI분석을 위한 피쳐분석 설계 및 구현
    - CAT검사와 비슷한 단어분석검사를 이용한 AI분석을 위한 피쳐분석 설계 및 구현
    - 아이들의 특정모습을 녹화 재생할수 있는 Animation 설계 및 구현
    - 관리자, 어린이, 부모용 로그인정보 DB설계 및 구현

● DB시스템 : PySpark를 기반으로 한 .DB시스템 설계 및 구축
    - PySpark Linux 서버 시스템 Build
    - PySpark를 활용한 데이터 레이크 및 데이터웨어하우스 설계 및 구현
    - PySpark를 활용한 정규표현식 기반의 정제 코드 빌드
    - PySpark기반의 Mongo DB 설계 및 구현
    - PySpark 데이터전송 코드 빌드
    - PySpark 실시간 스트리밍전송 시스템 설계 및 구현
    - 음성인식을 텍스트로 형태소 분리 저장하는 시스템 설계 및 구현

● 빅데이터 및 AI 플랫폼 : AI를 기반으로 한 분석 및 데이터빌드 플랫폼 구축
    - CNN 기반의 영상분석 및 자기학습 시스템 설계 및 구현
    - Regression 알고리즘을 적용한 ML 설계 및 구현
    - Classification 알고리즘을 적용한 ML 설계 및 구현
    - Clustering 알고리즘을 적용한 ML 설계 및 구현
    - ML알고리즘에 Dimensionality reduction 설계 및 구현
    - 기능 및 특성별 Model selection 설계 및 구현
    - 데이터의 효율성과 전자동화를 위한 Preprocessing 설계 및 구현
    - TensorFlow를 활용한 분산 데이터를 사용한 머신러닝 설계 및 구현
    - 구글코랩을 활용한 ML(DL) 설계 및 구현
    - Supervised, Unsupervised, Reinforcement, Deep Learning을 기반으로한 학습모델 및 예측모델 설계 및 구현

● 내가 담당하였던 프로젝트 업무

    - pyspark로 data streaming을 설계 구축, 데이터 레이크에 저장
    - 데이터 레이크에 저장된 데이터를 정규식으로 정제, 필터링
    - 데이터 레이크에서 음성인식을 텍스트로 변환한 데이터를 형태소로 분석, 분리
    - 데이터 레이크에서 정제된 데이터를 MongoDB에 데이터 웨어하우스로서 저장


    - Hadoop의 개념을 pySpark에 적용하여 개념적인 원리를 적용 
      빅데이터를 다룰 때 가장 일반적으로 쓰이는 기술은 하둡의 맵리듀스(MapReduce)와 연관 기술인 하이브(Hive) 인데 Hadoop보다 최근에 스파크가 업그레이드 되면서 나온 것이 파이 스파크이다. 대규모 데이터를 처리하여 분산, 저장한다고 하며, 빠른 처리와 높은 데이터 처리량이 인상적이었다. 그리고 내가 원하는 방법으로 데이터를 가공할 수 있었으며 visualization에도 인상적으로 처리하였다.

    - pyspark로 data streaming을 설계 구축, 데이터 레이크에 저장
      로봇에게서 나온 데이터를 실시간으로 들어게 하는 것이 우리의 목표였다. 그것을 할 수 있게 document를 찾고, 인터넷을 뒤져가며 찾아 보다가 문득 스트리밍이 어떻게 처리되는지 그 개념을 모르고 있다는 생각이 들어 더 공부하게 되었다. 내가 여기서 쓰는 스파크에서 스트리밍은, 데이터가 계속적으로 생성되어서 쌓이는 데이터를 말한다. 그러면 SprarkStreaming은 이 데이터를 배치 주기를 짧게 나누어서 실시간인 것 처럼 처리하는 것을 말한다. 그래서 연속된 데이터 스트림을 Districted Stream 또는 DStream이라고 하며 일련의 RDD로 표현된다. 그래서 Dstream은 연속된 데이터를 나타내기 위한 추상모델로, 데이터를 읽어내어서 spark에서 사용할 데이터 모델 인스턴스를 생성하게 되며, 여러가지 소스로부터 Dstream을 생성할 수 있게 별도의 메서드를 지원하고 있다. 
      그래서 코딩시에는 이 개념을 충실히 따라 줄 클래스로 SparkContext를 import한다. 그리고 스트리밍 모듈을 사용하려면 StreammingContext 인스턴스를 생성해야 한다. 이 StreamingContext는 특징이 있는데 첫 번째로 스트리밍의 시작 메서드, 대기 메서드, 종료 메서드가 있다는 것이다. 그리고 두번째로는 StreamingContext가 시작되면 새로운 연산의 정의/추가 불가능하다는 것이며, 세 번재로는 streaming이 종료되면 다시 시작은 불가능하다는 것이다. 그런데 스트리밍을 받다 보면 수 많은 중복된 데이터를 만나게 된다. 이런 중복된 데이터는 저장 공간을 효율적으로 다스려야 하는 서버의 입장에서는 골치거리가 된다. 우선 streaming의 처리과정은 이러하다. 일정 시간마다 stream은 잘려나가서 DStream(spark document에는 이를 original DStream이라고 한다)이 되고, 이를 windowed DStream이 일정시간 동안 몇 개의 DStream을 모아 가져가게 된다. 그리고 중복되는 것이 있는 것을 windowed DStream이 평가하게 되는데 이 때 쓰는 것이 앞서 설명한 hadoop의 mapreduce가 여기서 나오는 것이다. 
      그리고 이를 'data lake'라는 개념으로서 보관할 데이터베이스가 필요하다. spark 내에서는 이를 수행할 방법으로 saveAsTextFile() 같은 RDD의 형태로 보관한다. 그러나 이것으로 AI가 학습할 수 있는 것은 아니다. 왜냐하면 데이터는 mapReduce로 줄였어도 우리가 목적으로 하는 AI의 학습 자료가 되는 것은 아니다. 필요 없는 데이터들이 많기 때문이다. 그래서 data lake나 data warehouse나 같은 database라 할지라도 data lake는 다음의 데이터 정제과정을 기다리는 하나의 관문일 뿐 ai가 학습
      할 자료가 있는 database는 아니다. 

    - 받아들인 데이터를 정규식으로 데이터를 정제 필터링 & 형태소 분리
    빅데이터를 다룰 때 항상 필요한 것이 우리에게 어떤 정보가 필요한가를 정확히 아는 것이 중요했다. 내가 경험한 것은 엄청난 데이터를 거르는 알고리즘이 필요한데 이것이 정규식(Regular Expression)이다. 정규식은 우선 우리가 평소에 쓰지 않는 키보드 위의 문자를 활용한 것인데, 이를 메타문자라 한다. 언뜻 보면 외계어이다. 
    내가 먼저 배운 정규식은 (\d{1,6})[-]\d{1,7}이라는 이상한 정규식이었다. 하나하나 파 보기로 했다. \d는 영어의 Digit, 즉 숫자를 의미하는 것이다. 그런데 흥미로운 것은 \D라고 표기하면 오히려 not Digit, 즉 숫자가 아닌 모든 문자를 의미하는 것이었다. 그리고 비슷한 예로는 \w는 영어의 word, 즉 모든 단어를 의미하지만 \W라고 하면 not word, 단어가 아닌 것(숫자나 특수기호)를 의미하는 것이다. \s라고 하면 space bar를 의미하는 것으로 space bar가 입력된 것을 다 찾는 것이지만 이것 보다는 \S를 찾아서 한 어절을 구분하는 것을 찾는 것이다. 우리도 그렇게 문장의 어절을 쪼개어 가공할 때 \S으로 한 어절씩 쪼개게 되었다. 
    여기까지는 문자에 관한 것이고 다음에는 반복에 관한 것이다. 내가 먼저 배운 정규식의 이야기로 돌아가서 보면{1,6}이 의미하는 바는 앞의 \d의 반복이다. 즉 숫자 중에서 하나만 나오는 것이 \d의 의미라면 \d{1,6}의 의미는 숫자 중에서 여섯 번을 반복해서 나온다고 나는 이해하였다. 이곳에 중괄호 외에 반복을 뜻하는 메타문자는 .(반점), *(곱하기 기호), +(플러스 기호) 이 있다. 
    반점(.)는 줄 바꿈 문자\n을 제외한 모든 문자가 반점 자리와 매치될 수 있다. a.b를 예를 들면 abb, aeb 같이 a와 b사이에 어떤 문자가 들어와도 상관없이 하나의 문자가 들어갈 수 있다. 그리고 곱하기 기호(*)는 이 기호의 왼쪽의 문자들 중 하나가 반복적으로 수에 상관없이 매치된다. 단 곱하기 기호에 0개의 문자가 매치될 수도 있다. 예를 들어서 a*b라고 하면 aaaab, aab도 되지만 ab도 된다.
    플러스 기호(+)는 곱하기 기호는 같은 반복이더라도 딱 하나가 곱하기 기호(*)와 다르다. 곱하기 기호가 수에 상관없이 자신의 왼쪽에 있는 문자를 반복하는 것이라면 플러스 기호(+)는 최소 한 개를 보장한 수에 상관없이 문자를 반복하여 검사한다는 것이다. ca+b라고 하면 caab, caaaaab는 곱하기 기호(*)와 같이 검사하지만 cb는 매치되지 않는다. 
    내가 처음 접한 정규식에서 보이는 {1,6}는 그 왼쪽에 있는 것을 한 번에서 여섯 번을 반복하라는 뜻이다. 그렇게 하면 앞의 숫자(\d)와 어우러져서 주민번호 앞 여섯 자리를 쉽게 검출할 수 있다. 물론 {6}이라고 쓸 수도 있다. 그러나 의미는 왼쪽의 숫자가 꼭 여섯 번을 반복하여야 한다는 것이다. 그러나 이와 다르게 {,6} 라고 하면 6번 이하, {1,} 라고 하면 1번 이상 반복하라는 뜻이다.
    반복은 아니지만 ?(물음표 기호)도 있다. 물음표 기호(?)는 기존의 반복을 의미하는 그것들과는 다르게 물음표 기호(?) 왼쪽에 있는 것이 있든, 없든 상관없다는 기능이다. 예를 들어서 ab?c라면 abc도 매치되지만 ac도 출력된다. 
    그리고 반복은 아니지만 검사하는 방법으로 쓰이는 기호는 |(파이프 기호), ^(넌 뭐지?), $(달러 기호), [](대문자 기호)가 있다. 
    파이프 기호(|)는 코딩에서 자주 보던 or의 뜻이다. 조금 있다가 [](대문자 기호)와 같이 설명하면 이해가 빠를 것이다. 
    대문자 기호([]) 안에 들어온 것은 그 안에 나온 것은 한 자, 한 자 찾아 달라는 것이다. [y,Y]esterday를 예를 들어보면 Yesterday 또는 yesterday가 검출되어서 나올 것이다. Y와 y가 대괄호 안에 있기 때문이다. [a-zA-Z]라고 쓰면 a부터 z까지 대문자 소문자 모두 검색되는 것이니 아까 본 \w와 같이 되고, [0-9]라고 하게 되면 0 부터 9까지 검색되니 \d와 같이 되는 것이다. 그러나 여기서 [1-3| 9]가
    되면 1부터 3까지 그리고 9를 검색하라는 말이 된다. 
    여기에 넌 뭐지(^)가 추가가 되면 의미가 더 풍성해진다. 코딩시의 not의 뜻이며 이렇게 사용된다. [^0-9]라고 하면 숫자가 아닌 것을 고르라는 \D의 의미와 같다. 이런 식의 의미가 풍성해진다. 
    RegEx라는 사이트에 들어가면 이런 정규식을 연습할 수 있는 곳이다. 그런데 내가 재미있게 쓴 것은 .+라고 쓰면 문장 모두가 블록처리되면서 전체가 선택되었다고 나온다. 잘 쓰면 문장 자체를 거를 때 좋겠다고 생각했다.

    - 정제 된 데이터를 MongoDB에 데이터 웨어하우스로 저장.
    정제된 데이터를 처리 후 . 이 전에는 low-data를 저장 할 곳(data lake)가 필요하였다면 이것을 ai가 학습할 데이터로 만들어 저장할 곳이 필요했다. 최근 유명세를 가진 MongoDB가 우리의 선택을 받았는데, 왜냐하면 비정형 데이터를 다룰 수 있다는 점에 큰 점수를 주었다. 그러나 NoSQL이라는 것이 나를 당혹하게 만들지는 몰랐다. NoSQL이라는 특징이 Schema가 없다는 것이다. 그러면 여기에는 관계형 데이터베이스의 select문도 없다는 것인가? 개념이 잡히지 않는 부분이었다. 
    우선 내가 먼저 배운 database 중에는 mariaDB나 Oracle에서는 table이라는 것을 이곳에서는 collection이라고 불렀다. 또한 mariaDB나 Oracle에서 database라는 것을 MongoDB에서는 Document라고 불렀다. 또한 mongoDB는 python의 dict 타입처럼 key와 value값이 쌍으로 배치가 되어 있었다. 그래서 생소한 중괄호가 넘쳐나는 신기한 출력을 커맨드 창에서 볼 수 있다. 그리고 내가 배웠던 관계형 데이터베이스에는 한 column에 하나의 타입이 들어가는 방법과는 달리 python의 list처럼 대괄호([])로 여러 개의 값들이 뭉쳐 들어가는 것을 볼 수 있었다. 
    지금까지는 약과다. 이제 이것을 어떻게 데이터베이스 모델링을 해야 한다. 다행히도 사람들의 신상정보와 아이디, 비밀번호는 아까 배운 key, value값으로 만들 수 있었고, 그건 당연히 데이터베이스에 들어가는 것을 할 수 있었다. 그런데 그 다음부터가 문제였다. 그것을 바탕으로 SQL의 join처럼 다른 테이블을 결합해서 한번에 나타내는 방법을 몰랐다. 그런데 찾은 것은 embedded document라는 기능으로 join문과 비슷한 기능을 한다는 것을 알게 되었다. 그러니까 학생이라는 collection 아래에 온도를 측정한 것을 하나하나 쌓아 나가면 되는 일이었다. 